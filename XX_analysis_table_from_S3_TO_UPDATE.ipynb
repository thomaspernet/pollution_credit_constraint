{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform existing table and pre analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsPy.aws_authorization import aws_connector\n",
    "from awsPy.aws_s3 import service_s3\n",
    "from awsPy.aws_glue import service_glue\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os, shutil, json\n",
    "import sidetable\n",
    "\n",
    "\n",
    "path = os.getcwd()\n",
    "parent_path = str(Path(path).parent)\n",
    "\n",
    "\n",
    "name_credential = 'XXX.csv'\n",
    "region = ''\n",
    "bucket = ''\n",
    "path_cred = \"{0}/creds/{1}\".format(parent_path, name_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = aws_connector.aws_instantiate(credential = path_cred,\n",
    "                                       region = region)\n",
    "client= con.client_boto()\n",
    "s3 = service_s3.connect_S3(client = client,\n",
    "                      bucket = bucket, verbose = False)\n",
    "glue = service_glue.connect_glue(client = client,\n",
    "                      bucket = bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_setting = True\n",
    "if pandas_setting:\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and analyse data\n",
    "\n",
    "In the first part of the notebook, we will transform and create the data catalog and put the data in the `README.md`. In the second part of the notebook, we will analyse the data. Data analysis contains categorical and continuous variables. It is a batch analysis, nothing should done.\n",
    "\n",
    "# Download data locally\n",
    "\n",
    "First of all, load the data locally. Use the function `list_all_files_with_prefix` to parse all the files in a given folder. Change the prefix to the name of the folder in which the data are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'DATA/RAW_DATA'\n",
    "LOCAL_PATH_CATALOGUE= os.path.join(str(Path(path).parent.parent),'00_data_catalogue')\n",
    "LOCAL_PATH_CONFIG_FILE = os.path.join(str(Path(path).parent.parent),\n",
    "                                          '00_data_catalogue',\n",
    "                                          'temporary_local_data'\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = False\n",
    "if to_download:\n",
    "    FILES_TO_UPLOAD = s3.list_all_files_with_prefix(prefix=prefix)\n",
    "    list(\n",
    "        map(\n",
    "            lambda x:\n",
    "            s3.download_file(key=x, path_local=LOCAL_PATH_CONFIG_FILE),\n",
    "            FILES_TO_UPLOAD\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data\n",
    "\n",
    "\n",
    "## Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data catalog\n",
    "\n",
    "The data catalogue is a json file that we save in the folder `schema`. The schema is the following:\n",
    "\n",
    "```\n",
    "{\n",
    "        \"Table\": {\"Name\": \"\", \"StorageDescriptor\": {\"Columns\": [], \"Location\": \"\"}}\n",
    "    }\n",
    "``` \n",
    "\n",
    "The schema is automatically detected and generated from `FILES_TO_UPLOAD`. Since we don't know in advance the field, we cannot add comments at first. To add comments, please refer to the next part. \n",
    "\n",
    "### Create and save data catalog\n",
    "\n",
    "The schemas are saved locally in `schema/FILENAME`. Push the schema to GitHub for availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_schema(filename, extension = 'csv'):\n",
    "    \"\"\"\n",
    "    Prepare a json which is similar to glue schema.\n",
    "    It includes table name, columns, and path to S3\n",
    "\n",
    "    Output saved in schema/FILENAME\n",
    "    ARGS:\n",
    "\n",
    "    filename: string. filename of the doc to get the schema\n",
    "    extension:  Inform whether it's an Excel or CSV\n",
    "        - csv or excel\n",
    "    \"\"\"\n",
    "\n",
    "    schema_ = {\n",
    "        \"Table\": {\"Name\": \"\", \"StorageDescriptor\": {\"Columns\": [], \"Location\": {'s3URI':\"\", 's3Bucket': ''}}}\n",
    "    }\n",
    "    \n",
    "    if extension not in ['csv', 'excel']:\n",
    "        print('{} is not an accepter option. Please use excel or csv'.format(extension))\n",
    "        return extension\n",
    "\n",
    "    if extension == 'csv':\n",
    "        temp = pd.read_csv(filename)\n",
    "    elif extension == 'excel':\n",
    "        temp = pd.read_excel(filename)\n",
    "        \n",
    "    schema = pd.io.json.build_table_schema(temp)\n",
    "    schema_[\"Table\"][\"Name\"] = filename\n",
    "    schema_[\"Table\"][\"StorageDescriptor\"][\"Location\"]['s3URI'] = os.path.join(\n",
    "        \"s3://\", bucket, prefix, filename\n",
    "    )\n",
    "    schema_[\"Table\"][\"StorageDescriptor\"][\"Location\"]['s3Bucket'] = os.path.join(\n",
    "        \"https://s3.console.aws.amazon.com/s3\", bucket, prefix, filename\n",
    "    )\n",
    "    for i, name in enumerate(schema[\"fields\"]):\n",
    "        col = {\"Name\": name[\"name\"], \"Type\": name[\"type\"], \"Comment\": \"\"}\n",
    "        schema_[\"Table\"][\"StorageDescriptor\"][\"Columns\"].append(col)\n",
    "        \n",
    "    LOCAL_PATH_CONFIG_FILE = os.path.join(str(Path(path).parent.parent),\n",
    "                                          '00_data_catalogue',\n",
    "                                          'schema'\n",
    "                                     )\n",
    "\n",
    "    path_name = os.path.join(LOCAL_PATH_CONFIG_FILE, os.path.splitext(filename)[0])\n",
    "    with open(\"{}.json\".format(path_name), \"w\") as outfile:\n",
    "        json.dump(schema_, outfile)\n",
    "        \n",
    "    return schema_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in enumerate(FILES_TO_UPLOAD):\n",
    "    table = os.path.split(value)[1]\n",
    "    schema = prepare_schema(table)\n",
    "    print(json.dumps(schema, indent=4, sort_keys=False, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add comment\n",
    "\n",
    "This part is optional but strongly recommended. In this part, you are free to add any comment you need. To add a comment, alter the metadata of the file you want. To modify the comment, please, use:\n",
    "\n",
    "```\n",
    "[\n",
    "   {\n",
    "      \"Name\":\"\",\n",
    "      \"Type\":\"\",\n",
    "      \"Comment\":\"\"\n",
    "   }\n",
    "]\n",
    "```\n",
    "\n",
    "Fill only the variables you need to alter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_schema_table(filename, schema, print_new_schema = False):\n",
    "    \"\"\"\n",
    "    database: Database name\n",
    "        table: Table name\n",
    "        schema: a list of dict:\n",
    "        [\n",
    "        {\n",
    "        'Name': 'geocode4_corr',\n",
    "        'Type': '',\n",
    "        'Comment': 'Official chinese city ID'}\n",
    "        ]\n",
    "    \"\"\"\n",
    "    \n",
    "    LOCAL_PATH_CONFIG_FILE = os.path.join(str(Path(path).parent.parent),\n",
    "                                          '00_data_catalogue',\n",
    "                                          'schema'\n",
    "                                     )\n",
    "    \n",
    "    path_name = '{}.json'.format(os.path.join(LOCAL_PATH_CONFIG_FILE, os.path.splitext(filename)[0]))\n",
    "    \n",
    "    with open(path_name, 'r') as fp:\n",
    "        parameters = json.load(fp)\n",
    "        \n",
    "    list_schema = parameters['Table']['StorageDescriptor']['Columns']\n",
    "    for field in list_schema:\n",
    "        try:\n",
    "            field['Comment'] = next(\n",
    "                    item for item in schema if item[\"Name\"] == field['Name']\n",
    "                )['Comment']\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    parameters['Table']['StorageDescriptor']['Columns'] = list_schema\n",
    "    path_name = os.path.join(LOCAL_PATH_CONFIG_FILE, os.path.splitext(filename)[0])\n",
    "    with open('{}.json'.format(path_name), \"w\") as outfile:\n",
    "        json.dump(parameters, outfile)\n",
    "        \n",
    "    if print_new_schema:\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_alter = ''\n",
    "new_schema = [\n",
    "   {\n",
    "      \"Name\":\"\",\n",
    "      \"Type\":\"\",\n",
    "      \"Comment\":\" \"\n",
    "   }\n",
    "]\n",
    "update_schema_table(filename = filename_to_alter, schema = new_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate README \n",
    "\n",
    "The README is generated from `FILES_TO_UPLOAD` and will parse all the schema is `schema/FILENAME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "github_repo = ''\n",
    "github_owner = ''\n",
    "template_toc = os.path.join(\"https://github.com\", github_owner, github_owner, \"tree/master/00_data_catalogue#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "README = \"\"\"\n",
    "# Data Catalogue\n",
    "\n",
    "## Table of content\n",
    "\n",
    "\"\"\"\n",
    "bottom = \"\"\n",
    "data_in_catalogue = []\n",
    "for file in glob.glob(os.path.join(LOCAL_PATH_CATALOGUE, \"schema\", \"*.json\")):\n",
    "    data_in_catalogue.append(file)\n",
    "for key, value in enumerate(data_in_catalogue):\n",
    "    filename = os.path.split(value)[1]\n",
    "    \n",
    "    with open(os.path.join(LOCAL_PATH_CATALOGUE,'schema', filename), 'r') as fp:\n",
    "        parameters = json.load(fp)\n",
    "    tb = pd.json_normalize(parameters['Table']['StorageDescriptor']['Columns']).to_markdown()\n",
    "    template = \"\"\"\n",
    "\n",
    "## Table {0}\n",
    "\n",
    "- Filename: {1}\n",
    "- Location: {2}\n",
    "- S3uri: `{3}`\n",
    "\n",
    "\n",
    "{4}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    filename_no_extension = os.path.splitext(filename)[0]\n",
    "    filename_extension = parameters['Table']['Name']\n",
    "    location = parameters['Table']['StorageDescriptor']['Location']['s3Bucket']\n",
    "    uri = parameters['Table']['StorageDescriptor']['Location']['s3URI']\n",
    "    toc = '\\n- [{1}]({0}{1})'.format(template_toc, filename_no_extension)\n",
    "    README += toc\n",
    "    bottom += template.format(filename_no_extension, filename_extension,location, uri, tb)\n",
    "README += bottom    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_readme =os.path.join(LOCAL_PATH_CATALOGUE, \"README.md\")\n",
    "with open(path_readme, \"w\") as outfile:\n",
    "    outfile.write(README)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "The notebook file already contains code to analyse the dataset. It contains codes to count the number of observations for a given variables, for a group and a pair of group. It also has queries to provide the distribution for a single column, for a group and a pair of group. The queries are available in the key `ANALYSIS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Description\n",
    "\n",
    "During the categorical analysis, we wil count the number of observations for a given group and for a pair.\n",
    "\n",
    "**Count obs by group**\n",
    "\n",
    "- Index: primary group\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- percentage: Percentage of observation per primary group value over the total number of observations\n",
    "\n",
    "Returns the top 20 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILENAME 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(os.path.split(FILES_TO_UPLOAD[0])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the values fior each object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ = {'var': [],\n",
    "       'count':[],\n",
    "       'values': []}\n",
    "for v in df_test.select_dtypes(include='object').columns:\n",
    "    cat = df_test[v].nunique()\n",
    "    value_cat  = df_test[v].unique()\n",
    "    dic_['var'].append(v)\n",
    "    dic_['count'].append(cat)\n",
    "    dic_['values'].append(value_cat)\n",
    "(pd.DataFrame(dic_)\n",
    " .sort_values(by = ['count'], ascending = False)\n",
    " .set_index('var')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.concat([\n",
    "    df_test.isna().sum().sort_values().rename(\"count\"),\n",
    "    (df_test.isna().sum().sort_values()/len(df_test)).rename(\"pct\")\n",
    "    ], axis = 1\n",
    "    ).loc[lambda x: x['count']!=0]\n",
    "    .style\n",
    "    .format(\"{0:,.2%}\", subset=[\"pct\"], na_rep=\"-\")\n",
    "    .bar(subset=[\"count\"], color=\"#d65f5f\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(include=[\"string\", \"object\"]).columns):\n",
    "    df_count = df_test.stb.freq([objects])\n",
    "    if df_count.shape[0] > 20:\n",
    "        df_count = df_count.iloc[:20, :]\n",
    "    display(\n",
    "        (\n",
    "            df_count.reset_index(drop=True)\n",
    "            .style.format(\n",
    "                \"{0:,.2%}\", subset=[\"Percent\", \"Cumulative Percent\"], na_rep=\"-\"\n",
    "            )\n",
    "            .bar(subset=[\"Cumulative Percent\"], color=\"#d65f5f\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count obs by two pair\n",
    "\n",
    "You need to pass the primary group in the cell below\n",
    "\n",
    "- Index: primary group\n",
    "- Columns: Secondary key -> All the categorical variables in the dataset\n",
    "- nb_obs: Number of observations per primary group value\n",
    "- Total: Total number of observations per primary group value (sum by row)\n",
    "- percentage: Percentage of observations per primary group value over the total number of observations per primary group value (sum by row)\n",
    "\n",
    "Returns the top 20 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(include=[\"string\", \"object\"]).columns):\n",
    "    if objects not in [primary_key]:\n",
    "        df_count = df_test.stb.freq([objects])\n",
    "        if df_count.shape[0] > 20:\n",
    "            df_count = df_count.iloc[:20, :]\n",
    "        display(\n",
    "            (\n",
    "                df_test.stb.freq([primary_key, objects])\n",
    "                .set_index([primary_key, objects])\n",
    "                .drop(columns=['Cumulative Count', 'Cumulative Percent'])\n",
    "                .iloc[:20, :]\n",
    "                .unstack(-1)\n",
    "                .style\n",
    "                .format(\n",
    "                    \"{0:,.2%}\", subset=[\"Percent\"], na_rep=\"-\"\n",
    "                )\n",
    "                .format(\n",
    "                    \"{0:,.2f}\", subset=[\"Count\"], na_rep=\"-\"\n",
    "                )\n",
    "                .background_gradient(\n",
    "                    cmap=sns.light_palette(\"green\", as_cmap=True), subset=(\"Count\")\n",
    "                )\n",
    "\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous description\n",
    "\n",
    "There are three possibilities to show the ditribution of a continuous variables:\n",
    "\n",
    "- Display the percentile\n",
    "- Display the percentile, with one primary key\n",
    "- Display the percentile, with one primary key, and a secondary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_alter = \"\"\n",
    "df_test = pd.read_excel(filename_to_alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_test\n",
    "    .describe()\n",
    "    .style.format(\"{0:.2f}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Display the percentile, with one primary key\n",
    "\n",
    "The primary key will be passed to all the continuous variables\n",
    "\n",
    "- index: \n",
    "    - Primary group\n",
    "    - Percentile [.25, .50, .75, .95, .90] per primary group value\n",
    "- Columns: Secondary group\n",
    "- Heatmap is colored based on the row, ie darker blue indicates larger values for a given row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objects in list(df_test.select_dtypes(exclude=[\"string\", \"object\", 'boolean', 'datetime64[ns]']).columns):\n",
    "    if objects not in [primary_key]:\n",
    "        \n",
    "        print(\"\\nDistribution of {} by {}\\n\".format(objects, primary_key))\n",
    "        \n",
    "        display(\n",
    "            (\n",
    "                df_test\n",
    "                .groupby(primary_key)\n",
    "                .describe()[objects]\n",
    "                .sort_values(by='count', ascending=False)\n",
    "                .iloc[:20, :]\n",
    "                .style.format(\"{0:.2f}\")\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "In this section, we are going to perform:\n",
    "\n",
    "- Chi square test\n",
    "- Anova test\n",
    "\n",
    "To see if there is any dependence between the primary key, and the other variables.\n",
    "\n",
    "Each statistic is saved in the folder `statistical_analysis`\n",
    "\n",
    "### Chi square test\n",
    "\n",
    "There are two types of chi-square tests. Both use the chi-square statistic and distribution for different purposes:\n",
    "\n",
    "- A chi-square goodness of fit test determines if a sample data matches a population. For more details on this type, see: Goodness of Fit Test.\n",
    "- A chi-square test for independence compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each another.\n",
    "    - A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship.\n",
    "    - A very large chi square test statistic means that the data does not fit very well. In other words, there isn’t a relationship\n",
    "    \n",
    "The formula for the chi-square statistic used in the chi square test is:\n",
    "\n",
    "$$\n",
    "\\chi_{c}^{2}=\\sum \\frac{\\left(O_{i}-E_{i}\\right)^{2}}{E_{i}}\n",
    "$$\n",
    "\n",
    "The subscript $c$ are the degrees of freedom. $O$ is your observed value and $E$ is your expected value.\n",
    "\n",
    "A low value for chi-square means there is a high correlation between your two sets of data. In theory, if your observed and expected values were equal (\"no difference\") then chi-square would be zero — an event that is unlikely to happen in real life\n",
    "\n",
    "### Anova\n",
    "\n",
    "An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help you to figure out if you need to reject the null hypothesis or accept the alternate hypothesis.\n",
    "\n",
    "Basically, you’re testing groups to see if there’s a difference between them. Examples of when you might want to test different groups:\n",
    "\n",
    "- A group of psychiatric patients are trying three different therapies: counseling, medication and biofeedback. You want to see if one therapy is better than the others.\n",
    "- A manufacturer has two different processes to make light bulbs. They want to know if one process is better than the other.\n",
    "Students from different colleges take the same exam. You want to see if one college outperforms the other.\n",
    "\n",
    "Source: \n",
    "\n",
    "- [Chi-square](https://www.statisticshowto.com/probability-and-statistics/chi-square/)\n",
    "- [Anova](https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/anova/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A README is automatically generated, and is available at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(\"https://github.com\", github_owner, github_owner, \"tree/master/00_data_catalogue/statistical_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, use 10% probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_tables = {}\n",
    "\n",
    "to_include_cat = []\n",
    "to_include_cont = []\n",
    "\n",
    "feat_obj = list(df_test.select_dtypes(include=['object']))\n",
    "feat_cont = list(df_test.select_dtypes(\n",
    "    exclude=[\"string\", \"object\", 'boolean', 'datetime64[ns]']))\n",
    "\n",
    "readme_chi_square_middle_1 = \"\"\"\n",
    "\n",
    "# Chi square\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "readme_anova_middle_1 = \"\"\"\n",
    "\n",
    "# Anova\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# CHI SQUARE\n",
    "\n",
    "for col in feat_obj:\n",
    "    table = pd.crosstab(df_test[primary_key],\n",
    "                        df_test[col],\n",
    "                        margins=False)\n",
    "    if table.shape[1] > 1:\n",
    "        stat, p, dof, expected = chi2_contingency(table)\n",
    "        critical = chi2.ppf(proba, dof)\n",
    "\n",
    "        if abs(stat) >= critical:\n",
    "            to_include_cat.append('PO Sub Type')\n",
    "            result = 'Dependent (reject H0)'\n",
    "            to_include_cat.append(col)\n",
    "        else:\n",
    "            result = 'Independent (fail to reject H0)'\n",
    "\n",
    "        dic_results = {\n",
    "            'test': 'Chi Square',\n",
    "            'primary_key': primary_key,\n",
    "            'secondary_key': col,\n",
    "            'statistic': stat,\n",
    "            'p_value': p,\n",
    "            'dof': dof,\n",
    "            'critical': critical,\n",
    "            'result': result\n",
    "        }\n",
    "\n",
    "        dic_tables[col] = dic_results\n",
    "\n",
    "        # Tables\n",
    "        total_obs = table.sum(axis=0).sum()\n",
    "        cont_table = (\n",
    "            table.assign(total_rows=lambda x: x.sum(axis=1))\n",
    "            .append(table.sum(axis=0).rename('total_columns'))\n",
    "            .fillna(total_obs)\n",
    "        )\n",
    "        dic_contengency = {\n",
    "\n",
    "            'contengency': cont_table.to_json(),\n",
    "            'pearson_residual': ((table - expected) / np.sqrt(expected)).to_json(),\n",
    "            'pct_row': (table.apply(lambda r: r / r.sum(), axis=1)).to_json(),\n",
    "            'pct_columns': (table.apply(lambda r: r / r.sum(), axis=0)).to_json(),\n",
    "            'pct_total': (table.apply(lambda r: r / total_obs)).to_json()\n",
    "        }\n",
    "\n",
    "        path_name = os.path.join(\n",
    "            LOCAL_PATH_CATALOGUE, \"statistical_analysis\", 'chi-square', col.replace('/', ''))\n",
    "        with open('{}.json'.format(path_name), \"w\") as outfile:\n",
    "            json.dump(dic_contengency, outfile)\n",
    "\n",
    "        if cont_table.shape[1] > 20:\n",
    "            cont_table = cont_table.iloc[:, np.r_[:10, -10:-1, -1]]\n",
    "            is_full = 'Troncated, only first/last 10 columns'\n",
    "        else:\n",
    "            is_full = 'Full table'\n",
    "        readme_chi_square_middle_2 = \"\"\"\n",
    "\n",
    "### {0}\n",
    "\n",
    "- Results between {0} and {1}: {2}\n",
    "- Contengency table ({4}):\n",
    "\n",
    "{3}\n",
    "\n",
    "        \"\"\".format(col, primary_key, result, cont_table.to_markdown(), is_full)\n",
    "\n",
    "        readme_chi_square_middle_1 += readme_chi_square_middle_2\n",
    "\n",
    "for col in feat_cont:\n",
    "    result = df_test.groupby(primary_key)[col].apply(list)\n",
    "    F, p = stats.f_oneway(*result)\n",
    "    if p <= 1 - proba:\n",
    "        result = 'Dependent (fail to reject H0)'\n",
    "        to_include_cont.append(col)\n",
    "        \n",
    "    else:\n",
    "        result = 'Independent (reject H0)'\n",
    "\n",
    "    dic_results = {\n",
    "        'test': 'Anova',\n",
    "        'primary_key': primary_key,\n",
    "        'secondary_key': col,\n",
    "        'statistic': F,\n",
    "        'p_value': p,\n",
    "        'result': result\n",
    "    }\n",
    "\n",
    "    dic_tables[col] = dic_results\n",
    "\n",
    "    readme_anova_middle_2 = \"\"\"\n",
    "    \n",
    "### {0}\n",
    "    \n",
    "- Results between {0} and {1}: {2}\n",
    "    \n",
    "    \"\"\".format(col, primary_key, result)\n",
    "\n",
    "    readme_anova_middle_1 += readme_anova_middle_2\n",
    "    \n",
    "full_table = (\n",
    "    pd.DataFrame(dic_tables).T\n",
    "    .sort_values(by = ['test', 'result'])\n",
    "    .assign(\n",
    "        statistic = lambda x: np.round(x['statistic'].astype('float'), 2),\n",
    "        dof = lambda x: np.round(x['dof'].astype('float'), 2),\n",
    "        critical = lambda x: np.round(x['critical'].astype('float'), 2),\n",
    "        p_value = lambda x: np.round(x['p_value'].astype('float'), 2),\n",
    "    )\n",
    "    #\n",
    ")\n",
    "\n",
    "readme_top = \"\"\"\n",
    "# Statistical Analysis \n",
    "\n",
    "The primary key is {0}\n",
    "\n",
    "The full results are listed below:\n",
    "\n",
    "{1}\n",
    "\n",
    "List of relevant variables:\n",
    "\n",
    "\"\"\".format(primary_key, full_table.fillna('-').to_markdown())\n",
    "\n",
    "# Save README\n",
    "to_include = to_include_cat + to_include_cont\n",
    "for i, val in enumerate(to_include):\n",
    "    relevant_var = \"{}. {}\\n\".format(i+1, val)\n",
    "    readme_top += relevant_var\n",
    "\n",
    "path_readme = os.path.join(\n",
    "    LOCAL_PATH_CATALOGUE, 'statistical_analysis', \"README.md\")\n",
    "with open(path_readme, \"w\") as outfile:\n",
    "    outfile.write(readme_top + readme_chi_square_middle_1 +\n",
    "                  readme_anova_middle_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    full_table\n",
    "    .style\n",
    "                .format(\n",
    "                    \"{0:,.2%}\", subset=[\"p_value\"], na_rep=\"-\"\n",
    "                )\n",
    "                .format(\n",
    "                    \"{0:,.2f}\", subset=[\"statistic\", \"dof\", 'critical'], na_rep=\"-\"\n",
    "                )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize in more detail the contency table, you can use the function `contengency_table`. The function parses the folder `statistical_analysis/chi-square`. Five tables are generated:\n",
    "\n",
    "- Contengency table full: contengency\n",
    "- Pearson contribution: pearson_residual\n",
    "- Centengency table percentage row-wise: pct_row\n",
    "- Centengency table percentage column-wise: pct_columns\n",
    "- Centengency table percentage full: pct_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_contengency(filename, option='contengency', style=True):\n",
    "    \"\"\"\n",
    "    Read the contengency table\n",
    "    filename: Filename to load, including `.json`. \n",
    "    Check the folder `statistical_analysis/chi-square`  to get the name\n",
    "    \"\"\"\n",
    "    path_name = os.path.join(LOCAL_PATH_CATALOGUE,\n",
    "                             \"statistical_analysis\", 'chi-square', filename)\n",
    "\n",
    "    with open(path_name, 'r') as fp:\n",
    "        table = json.load(fp)\n",
    "\n",
    "    if option in ['pct_row', 'pct_columns', 'pct_total']:\n",
    "\n",
    "        table = pd.read_json(table[option])\n",
    "\n",
    "        if style:\n",
    "            table = (\n",
    "                table\n",
    "                .style\n",
    "                .format(\n",
    "                    \"{0:,.2%}\", na_rep=\"-\"\n",
    "                )\n",
    "                .background_gradient(\n",
    "                    cmap=sns.light_palette(\"green\", as_cmap=True)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return table\n",
    "    else:\n",
    "        table = pd.read_json(table[option])\n",
    "        if style:\n",
    "            table = (table.style\n",
    "                     .background_gradient(\n",
    "                         cmap=sns.light_palette(\"green\", as_cmap=True)\n",
    "                     )\n",
    "                     )\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "read_contengency(filename, option = 'pct_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "- Heatmap\n",
    "- Diverging bar\n",
    "- Scatter plot\n",
    "- Correspondance analysis\n",
    "\n",
    "### \n",
    "- heatmap, code by [Seaborn](https://seaborn.pydata.org/examples/many_pairwise_correlations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Generate a large random dataset\n",
    "d = df_test.select_dtypes(\n",
    "    exclude=[\"string\", \"object\", 'boolean', 'datetime64[ns]'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = d.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot\n",
    "\n",
    "From the correlation plot above, pick up a $y$ variables.\n",
    "\n",
    "We only plot the variables that succeed the Anova test, minus the $y$ var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = ''\n",
    "for i, col in enumerate(to_include_cont):\n",
    "    if col != y_var:\n",
    "        #plt.figure(i)\n",
    "        f, ax = plt.subplots(figsize=(7, 7))\n",
    "        ax.set(xscale=\"log\", yscale=\"log\")\n",
    "        (sns.regplot(x=col, y=y_var, data=df_test, ax=ax, scatter_kws={\"s\": 100})\n",
    "         .set_title('Scatterplot between {} and {}'.format(y_var, col))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diverging bars\n",
    "\n",
    "The diverging bar plot is plotting for the variables to succeed the Anova test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col  in to_include_cont:\n",
    "    df_test_ = df_test.groupby([primary_key])[col].mean().reset_index()\n",
    "    df_test_['mean'] = (df_test_[col] - df_test_[col].mean())/df_test_[col].std()\n",
    "    df_test_['colors'] = ['red' if x < 0 else 'green' for x in df_test_['mean']]\n",
    "    df_test_.sort_values('mean', inplace=True)\n",
    "    df_test_.reset_index(inplace=True)\n",
    "    # Draw plot\n",
    "    plt.figure(figsize=(14, 10), dpi=80)\n",
    "    plt.hlines(y=df_test_.index, xmin=0, xmax=df_test_['mean'],\n",
    "               color=df_test_['colors'], alpha=0.4, linewidth=5)\n",
    "    # Decorations\n",
    "    text = \"Diverging Bars of {} within {} \".format(col, primary_key)\n",
    "    plt.gca().set(ylabel=primary_key, xlabel=col)\n",
    "    plt.yticks(df_test_.index, df_test_[primary_key], fontsize=12)\n",
    "    plt.title(text, fontdict={'size': 20})\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correspondance analysis\n",
    "\n",
    "We created a Python library to make a correspondance analysis. Please, refers to [https://github.com/thomaspernet/Correspondence_analysis](https://github.com/thomaspernet/Correspondence_analysis/blob/master/CorrespondenceAnalysisPy/correspondence_analysis_computation/ca_compute.py) for the codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CorrespondenceAnalysisPy.correspondence_analysis_computation import ca_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in to_include_cat:\n",
    "    name = '{}.json'.format(var)\n",
    "    try:\n",
    "        tb = read_contengency(filename=name, option='contengency', style=False)\n",
    "        ca = ca_compute.compute_ca(\n",
    "            (\n",
    "                tb\n",
    "                .iloc[:-1, :-1]\n",
    "            )\n",
    "        )\n",
    "        ca_computed = ca.correspondance_analysis()\n",
    "        fig_2 = ca_compute.row_focus_coordinates(\n",
    "            df_x=ca_computed['pc_rows'],\n",
    "            df_y=ca_computed['pc_columns'],\n",
    "            variance_explained=ca_computed['variance_explained'],\n",
    "            export_data=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Generate reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "import os, time, shutil, urllib, ipykernel, json\n",
    "from pathlib import Path\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "def create_report(extension = \"html\", keep_code = False):\n",
    "    \"\"\"\n",
    "    Create a report from the current notebook and save it in the \n",
    "    Report folder (Parent-> child directory)\n",
    "    \n",
    "    1. Exctract the current notbook name\n",
    "    2. Convert the Notebook \n",
    "    3. Move the newly created report\n",
    "    \n",
    "    Args:\n",
    "    extension: string. Can be \"html\", \"pdf\", \"md\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Get notebook name\n",
    "    connection_file = os.path.basename(ipykernel.get_connection_file())\n",
    "    kernel_id = connection_file.split('-', 1)[0].split('.')[0]\n",
    "\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            if srv['token']=='' and not srv['password']:  \n",
    "                req = urllib.request.urlopen(srv['url']+'api/sessions')\n",
    "            else:\n",
    "                req = urllib.request.urlopen(srv['url']+ \\\n",
    "                                             'api/sessions?token=' + \\\n",
    "                                             srv['token'])\n",
    "            sessions = json.load(req)\n",
    "            notebookname = sessions[0]['name']\n",
    "        except:\n",
    "            pass  \n",
    "    \n",
    "    sep = '.'\n",
    "    path = os.getcwd()\n",
    "    #parent_path = str(Path(path).parent)\n",
    "    \n",
    "    ### Path report\n",
    "    #path_report = \"{}/Reports\".format(parent_path)\n",
    "    #path_report = \"{}/Reports\".format(path)\n",
    "    \n",
    "    ### Path destination\n",
    "    name_no_extension = notebookname.split(sep, 1)[0]\n",
    "    source_to_move = name_no_extension +'.{}'.format(extension)\n",
    "    dest = os.path.join(path,'Reports', source_to_move)\n",
    "    \n",
    "    ### Generate notebook\n",
    "    if keep_code:\n",
    "        os.system('jupyter nbconvert --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    else:\n",
    "        os.system('jupyter nbconvert --no-input --to {} {}'.format(\n",
    "    extension,notebookname))\n",
    "    \n",
    "    ### Move notebook to report folder\n",
    "    #time.sleep(5)\n",
    "    shutil.move(source_to_move, dest)\n",
    "    print(\"Report Available at this adress:\\n {}\".format(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "create_report(extension = \"html\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.23.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
